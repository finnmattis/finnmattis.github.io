# EvidenceBench: A Benchmark for Extracting Evidence from Biomedical Papers

EvidenceBench is a comprehensive benchmark designed to test Large Language Models (LLMs) ability to extract relevant evidence from biomedical papers. Finding relevant evidence is a necessary precursor for evaluating the validity of a scientific hypothosis, and for applications such as automated meta-anaylses and scientifically grounded question-answering systems. EvidenceBench includes over 400 fully annotated papers and 700,000 sentence judgments, leveraging expert annotations to match hypotheses with relevant evidence. The performance of the best models still falls significantly short of expert-level on this task. By providing a standardized benchmark and evaluation framework, this EvidenceBench will support the development of tools which automate evidence synthesis and hypothesis testing.

# Getting Started

* [Quickstart](https://finnmattis.github.io/quickstart.html)
* [Dataset Information](https://finnmattis.github.io/dataset.html)
* [Example Usage](https://finnmattis.github.io/example.html)

## References

* [EvidenceBench paper](https://arxiv.org/abs/2306.08754)